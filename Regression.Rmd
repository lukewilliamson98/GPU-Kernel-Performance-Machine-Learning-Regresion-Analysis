---
output:
  pdf_document: default
  html_document: default
---
# Luke Williamson 

"Project: SGEMM GPU kernel performance Data Set Regression"

#################################################################################################################
This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Ctrl+Shift+Enter*. 

Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
#################################################################################################################

# Raw Data Exploration
-Link for the data: https://archive.ics.uci.edu/ml/datasets/SGEMM+GPU+kernel+performance

Citations for the data:
- Rafael Ballester-Ripoll, Enrique G. Paredes, Renato Pajarola. 
Sobol Tensor Trains for Global Sensitivity Analysis. 
In arXiv Computer Science / Numerical Analysis e-prints, 2017 
https://arxiv.org/abs/1712.00233 
- Cedric Nugteren and Valeriu Codreanu. 
CLTune: A Generic Auto-Tuner for OpenCL Kernels. 
In: MCSoC: 9th International Symposium on Embedded Multicore/Many-core Systems-on-Chip. IEEE, 2015 
https://ieeexplore.ieee.org/document/7328205

-***After downloading the data, be sure to change the full file path name accordingly in the chunk below.***
-Read in file to dataframe and explore raw data:
```{r}
  df <- read.csv("C:/Users/law/Documents/sgemm_product.csv", header=TRUE)
  dim(df)
  names(df)
  str(df)
  head(df)
  tail(df)
  summary(df)
  sapply(df, class)
  sapply(df, function(x) sum(is.na(x)==TRUE))
```

# Cleaned Data Exploration
-To clean the data, I averaged the last 4 columns (which were 4 seperate trials (not attributes)) 
  over all instances and added the averages to a new column in the dataframe called run_avg.
-I then removed columns 15:18 which contained the 4 seperate runtime trials.
-I then convert columns 11:14 to factors, because we can see that in the above chunk, 
  R interpreted them as integers.
-Due to processing constraints, I took a random subsetted sample of size 10000 the dataset.
Cleaned dataframe and explore new data:
```{r}
  df$run_avg <- ((df$Run1..ms. + df$Run2..ms. + df$Run3..ms. + df$Run4..ms.) / 4)
  df[15:18] <- list(NULL)
  df$STRM <- as.factor(df$STRM)
  df$STRN <- as.factor(df$STRN)
  df$SA <- as.factor(df$SA)
  df$SB <- as.factor(df$SB)
  df <- df[sample(1:nrow(df), 10000, replace=FALSE),]
  dim(df)
  names(df)
  str(df)
  head(df)
  tail(df)
  summary(df)
  sapply(df, class)
```

# Establish train/test Split
```{r}
  set.seed(1234)
  i <- sample(1:nrow(df), 0.75*nrow(df), replace=FALSE)
  train <- df[i,]
  test <- df[-i,]
```

# Linear Regression 1
This Multiple Linear Regression model has a poor R^2, correlation, and MSE/RMSE. 
R^2 = 0.42, cor = 0.630, MSE = 86062, RMSE = 293
```{r}
  lm1 <- lm(run_avg~., data=train)
  summary(lm1)
  par(mfrow=c(2,2))
  plot(lm1)
  pred1 <- predict(lm1, newdata=test)
  cor(pred1, test$run_avg)
  mse1 <- mean((pred1-test$run_avg)^2)
  mse1
  rmse1 <- sqrt(mse1)
  rmse1
```

# Linear Regression 2
Next, I predicted the log of run_avg. I took away the 4 worst predictors (MDIMA, NDIMB, KWI, STRN) 
to get the best possible metrics using log. Although R^2 is better than the previous model, the correlation 
and MSE/RMSE are worse.
R^2 = 0.57, cor = 0.606, MSE = 186561, RMSE = 432
```{r}
  lm2 <- lm(log2(run_avg)~. -MDIMA-NDIMB-KWI-STRN, data=train)
  summary(lm2)
  par(mfrow=c(2,2))
  plot(lm2)
  pred2 <- predict(lm2, newdata=test)
  cor(pred2, test$run_avg)
  mse2 <- mean((pred2-test$run_avg)^2)
  mse2
  rmse2 <- sqrt(mse2)
  rmse2
```

# Linear Regression 3
Next, I jumped back to non-log linear regression and began varying my feature selections.
This is the best result I got (predicting based on all attributes except STRN).
lm1 still performs slightly better than lm3.
R^2 = 0.42, cor = 0.630, MSE = 86070, RMSE = 293
Still, not much is learned from the data as it is a poor model.
```{r}
  lm3 <- lm(run_avg~. -STRN, data=train)
  summary(lm3)
  par(mfrow=c(2,2))
  plot(lm3)
  pred3 <- predict(lm3, newdata=test)
  cor(pred3, test$run_avg)
  mse3 <- mean((pred3-test$run_avg)^2)
  mse3
  rmse3 <- sqrt(mse3)
  rmse3
```

# kNN Regression 1
This first chunk for kNN Regression is to establish a baseline for kNN Reg using k=3.
Initially, we can see that kNN is far superior to Linear Regression.
cor = 0.851, MSE = 40700, RMSE = 202
```{r}
  library(caret)
  library(DMwR)
  knn4 <- knnreg(run_avg~., data=train, k=3)
  summary(knn4)
  pred4 <- predict(knn4, newdata=test)
  cor(pred4, test$run_avg)
  mse4 <- mean((pred4-test$run_avg)^2)
  mse4
  rmse4 <- sqrt(mse4)
  rmse4
```

# kNN Regression 2 (feature selection)
In this chunk, the optimal k to use for kNN Regression is determined.
Due to processing constraints, I only tested for k=1,3,5,7,9.
k = 7 gave the highest cor = 0.864 and lowest mse = 36545, so k = 7 is out optimal feature.
```{r}
  k_cor <- rep(0,10)
  k_mse <- rep(0,10)
  i <- 1
  for(k in seq(1,19,2)) {
    knnk <- knnreg(run_avg~., data=train, k=k)
    predk <- predict(knnk, newdata=test)
    k_cor[i] <- cor(predk, test$run_avg)
    k_mse[i] <- mean((predk-test$run_avg)^2)
    print(paste("k =", k, k_cor[i], k_mse[i]))
    i <- i+1
  }
```

# kNN Regression 3
Now, we plot the k values to better visualize how our feature selection affects kNN.
```{r}
  plot(1:10, k_cor, lwd=2, col='blue', ylab="", yaxt='n')
  par(new=TRUE)
  plot(1:10, k_mse, lwd=2, col='green', labels=FALSE, ylab="", yaxt='n')
```


# SVM Regression 1 (linear kernel)
kNN with k=7 is still the best model, so polynomial and radial kernels will not make a significant difference.
cor = 0.588, MSE = 109672, RMSE = 331
```{r}
  library(e1071)
  set.seed(1234)
  svm5 <- svm(run_avg~., data=train, kernel="linear", cost=1, scale=FALSE)
  summary(svm5)
  pred5 <- predict(svm5, newdata=test)
  cor(pred5, test$run_avg)
  mse5 <- mean((pred5-test$run_avg)^2)
  mse5
  rmse5 <- sqrt(mse5)
  rmse5
```

# Decision Tree Regression
While correlation is lower than kNN's, MSE is much lower which is great. This is overall the best model
because it has a high correlation and a relatively low MSE/RMSE.
cor = 0.930, MSE = 19588, RMSE = 140
```{r}
  library(tree)
  tree6 <- tree(run_avg~., data=train)
  summary(tree6)
  pred6 <- predict(tree6, newdata=test)
  cor(pred6, test$run_avg)
  mse6 <- mean((pred6-test$run_avg)^2)
  mse6
  rmse6 <- sqrt(mse6)
  rmse6
```

# Analysis
-Decision Tree Regression has the highest correlation and lowet MSE/RMSE, thus it is the best performing 
algorithm out of linear regression, kNN regression, SVM, and decision tree regression. 
-If the data is simple, then decision tree regression will usually perform worse than other regression algorithms.
However, since our dataset is complex and decision tree regression is high variance/low bias, 
the algorithm performs better than the others.